{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "В цьому домашньому завданні ми проведемо додаткові експерименти для рішення задачі бінарної класифікації і створимо ваш новий submission на змагання на Kaggle.\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "**Завдання 0**. Завантажте дані `train.csv`, `test.csv`, `sample_submission.csv` зі змагання на Kaggle - шукайте посилання в уроці [Запрошення до участі у Kaggle-змаганні.](https://data-loves.kwiga.com/courses/machine-learning-dlia-liudei/domashnie-zavdannia-zmagannia-z-kaggle)  Для завантаження потрібно долучитись до змагання (натиснути кнопку \"Join\").\n",
    "\n",
    "\n",
    "**Завдання 1**. **Збираємо весь код з попереднього ДЗ в одному місці.** В лекційному ноутбуці `Логістична регресія з ScikitLearn. Повна ML задача.ipynb` ми познайомились з поняттям пайплайнів, а також я показала, як компактно виглядає рішення МЛ задачі, якщо ми зберемо весь код разом.\n",
    "\n",
    "Оскільки ми далі будемо робити експерименти, які включають ті самі етапи попередньої обробки, але інше моделювання - буде зручно мати весь код компактно і під рукою. Тому зараз ми займемось збором коду до купи :) Після цього завдання для подальших експериментів ви можете перенести частини розвʼязку взагалі в окремий `.py` файл, аби було зручно імпортувати функції.\n",
    "\n",
    "Зі свого рішення в попередньому домашньому завданні (`Логістична регресія з scikit learn.ipynb`) зберіть усі кроки розвʼязку задачі разом з використанням `sklearn.Pipeline` за прикладом з лекції.\n",
    "\n",
    "Ваш код нижче має містити\n",
    "1. Читання даних з файлу (поза пайплайном).\n",
    "2. Розбиття на тренувальний і валідаційний набори, де валідаційний містить 20% даних (поза пайплайном).\n",
    "3. Виділення категоріальних і числових колонок (поза пайплайном).\n",
    "4. Підготовку категоріальних і числових колонок (частина пайплайну). В прикладі в лекції ми оформлювали обробку числових і категоріальних колонок в окремі трансформери `numeric_transformer`, `categorical_cols`. Рекоемндую зробити саме так, так потім зручніше вносити зміни :)\n",
    "5. Тренування лог регресії (частина пайплайну).\n",
    "6. Запуск пайплайну на тренування на трен. даних (поза пайплайном).\n",
    "7. Запуск пайплайну на передбачення на трен і вал. даних і вимір метрик якості ROC-AUC + вивдення Confusion Matrix (поза пайплайном).\n",
    "8. Збереження моделі в формат joblib (поза пайплайном).\n",
    "\n",
    "Ви це все вже зробили в попереднтьому ДЗ! Тож, тут просто заадча все зібрати разом.\n",
    "\n",
    "Нижче я додала підказки, що покроково ви маєте зробити. Якщо ви почуваєтесь впевнено, можете видалити ці підказки і реалізувати все самостійно, або ж - просто заповнити пропуски.\n",
    "\n",
    "Завдання оцінюється в 10 балів. Головний результат - аби код в фіналі був робочий. Бо за не робочий нам гроші ніхто не заплатить :)"
   ],
   "metadata": {
    "id": "gJ2A6t3mdEed"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# index column id in dataset\n",
    "bank_df = pd.read_csv('/Users/macbook/Desktop/machine_learning_course/train.csv', index_col=0)\n",
    "\n",
    "# train and validation subsets\n",
    "# use stratify\n",
    "train_df, val_df = train_test_split(bank_df, test_size=0.2, random_state=42, stratify=bank_df['Exited'])\n",
    "\n",
    "# я дропала колонки У попередній д/р., відрізняється від розв'язку, який у вас. Залишила поки так\n",
    "target_col = 'Exited'\n",
    "drop_cols = ['id', 'CustomerId', 'Surname', target_col]\n",
    "input_cols = train_df.columns.difference(drop_cols).tolist()\n",
    "train_inputs = train_df[input_cols].copy()\n",
    "train_targets = train_df[target_col].copy()\n",
    "val_inputs = val_df[input_cols].copy()\n",
    "val_targets = val_df[target_col].copy()\n",
    "\n",
    "# Виявляємо числові і категоріальні колонки\n",
    "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = train_inputs.select_dtypes('object').columns.tolist()\n",
    "\n",
    "# Створюємо трансформери для числових і категоріальних колонок\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "# Комбінуємо трансформери для різних типів колонок в один препроцесор\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "    ])\n",
    "\n",
    "classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Стоврюємо пайплайн, який спочатку запускає препроцесинг, потім тренуєм модель\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier ),\n",
    "])\n",
    "\n",
    "# Тренуємо пайплайн\n",
    "model_pipeline.fit(train_inputs, train_targets)\n",
    "\n",
    "# Функція, щоб передбачати і рахувати метрики\n",
    "def predict_and_plot(model_pipeline, inputs, targets, name=''):\n",
    "    # predictions 0/1\n",
    "    preds = model_pipeline.predict(nputs)\n",
    "    # for ROC\n",
    "    probs = model_pipeline.predict_proba(inputs)[:, 1]\n",
    "    print(f\"F1-score: {f1_score(targets, preds):.2f}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(targets, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "    cf = confusion_matrix(targets, preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'.format(name))\n",
    "    plt.show()\n",
    "    return preds, probs\n",
    "\n",
    "# Оцінюємо модель на трен і вал даних\n",
    "train_preds = predict_and_plot(model_pipeline, train_inputs, train_targets, 'Train')\n",
    "val_preds, val_probs = predict_and_plot(model_pipeline, val_inputs, val_targets, 'Validation')\n",
    "\n",
    "# Зберігаємо модель для подальшого використання\n",
    "joblib.dump(model, 'log_reg.joblib')\n"
   ],
   "metadata": {
    "id": "0LZialdo4IPZ",
    "ExecuteTime": {
     "end_time": "2026-02-11T22:55:14.836277Z",
     "start_time": "2026-02-11T22:55:12.927967Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 38\u001B[0m\n\u001B[1;32m     35\u001B[0m categorical_cols \u001B[38;5;241m=\u001B[39m train_inputs\u001B[38;5;241m.\u001B[39mselect_dtypes(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Створюємо трансформери для числових і категоріальних колонок\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m numeric_transformer \u001B[38;5;241m=\u001B[39m Pipeline(steps\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     39\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscaler\u001B[39m\u001B[38;5;124m'\u001B[39m, MinMaxScaler()),\n\u001B[1;32m     40\u001B[0m ])\n\u001B[1;32m     42\u001B[0m categorical_transformer \u001B[38;5;241m=\u001B[39m Pipeline(steps\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     43\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124monehot\u001B[39m\u001B[38;5;124m'\u001B[39m, OneHotEncoder(handle_unknown\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m, sparse_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)),\n\u001B[1;32m     44\u001B[0m ])\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Комбінуємо трансформери для різних типів колонок в один препроцесор\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 2**. Такс, у нас з вами є вже готовий пайплайн. Давайте проведемо нові експерименти.\n",
    "\n",
    "  Додайте в попередню обробку числових колонок генерацію polinomal features до степені 2 включно. Для цього створіть новий препроцесор і створіть новий пайплайн.\n",
    "\n",
    "  Запустіть пайплайн на тренування і виведіть метрики для тренувального і валідаційного набору. Напишіть, як вам модель? Чи спостерігається в цій моделі overfit чи underfit? Чи ця модель добре генералізує?"
   ],
   "metadata": {
    "id": "PXrc2NCa5lAK"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TjcmWMTOOjJ1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 3**. Тепер давайте створимо ще новий пайплайн, тільки тепер поліноміальні ознаки згенеруємо до степені 4. Зробіть висновок про якість моделі. Якщо вам подобається резульат якоїсь з моделей в цьому ДЗ - рекомендую зробити submission в змаганні."
   ],
   "metadata": {
    "id": "tkmEmHaP8Pen"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OsT-MDWuOkDY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 4. Перенавчання і регуляризація**.\n",
    "\n",
    "  Скачайте набір даних `regression_data.csv`. Звичайте набір даних з `regression_data.csv`, розбийте на train і test (в тест 20%) і натренуйте модель лінійної регресії з масштабуванням числових ознак і поліноміальними ознаками до степені **5 включно**.\n",
    "\n",
    "  Виміряйте якість прогностичної моделі і зробіть висновок, чи модель хороша, чи вона добре генералізує?\n"
   ],
   "metadata": {
    "id": "ozN2ONZGCBS6"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xbl0jQ3WOlgn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання 5**. Натренуйте моделі Lasso(), Ridge(), ElasaticNet() на цих даних (з поліном ознаками до степені 20 включно), порівняйте якість з тою, яка була отримана з лінійною регресією. Яка модель найкраще генералізує і чому на ваш погляд (можливо треба буде для відповіді зробити додатковий аналіз ознак)?"
   ],
   "metadata": {
    "id": "JNUt-Q6UHkn7"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "y93ItPYdOnpE"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
